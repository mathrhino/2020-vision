{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# coding: utf-8\n",
        "\"\"\"\n",
        "Author:\n",
        "    Weichen Shen,wcshen1994@163.com\n",
        "\n",
        "Reference:\n",
        "    [1] Feng Y, Lv F, Shen W, et al. Deep Session Interest Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1905.06482, 2019.(https://arxiv.org/abs/1905.06482)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "from tensorflow.python.keras.initializers import RandomNormal\n",
        "from tensorflow.python.keras.layers import (Concatenate, Dense, Embedding,\n",
        "                                            Flatten, Input)\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "\n",
        "from deepctr.input_embedding import (create_singlefeat_inputdict,\n",
        "                               get_embedding_vec_list, get_inputs_list)\n",
        "from deepctr.layers.core import DNN, PredictionLayer\n",
        "from deepctr.layers.sequence import (AttentionSequencePoolingLayer, BiasEncoding,\n",
        "                               BiLSTM, Transformer)\n",
        "from deepctr.layers.utils import NoMask, concat_fun\n",
        "from deepctr.utils import check_feature_config_dict\n",
        "\n",
        "\n",
        "def DSIN(feature_dim_dict, sess_feature_list, embedding_size=8, sess_max_count=5, sess_len_max=10, bias_encoding=False,\n",
        "         att_embedding_size=1, att_head_num=8, dnn_hidden_units=(200, 80), dnn_activation='sigmoid', dnn_dropout=0,\n",
        "         dnn_use_bn=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, init_std=0.0001, seed=1024, task='binary',\n",
        "         ):\n",
        "    \"\"\"Instantiates the Deep Session Interest Network architecture.\n",
        "\n",
        "    :param feature_dim_dict: dict,to indicate sparse field (**now only support sparse feature**)like {'sparse':{'field_1':4,'field_2':3,'field_3':2},'dense':[]}\n",
        "    :param sess_feature_list: list,to indicate session feature sparse field (**now only support sparse feature**),must be a subset of ``feature_dim_dict[\"sparse\"]``\n",
        "    :param embedding_size: positive integer,sparse feature embedding_size.\n",
        "    :param sess_max_count: positive int, to indicate the max number of sessions\n",
        "    :param sess_len_max: positive int, to indicate the max length of each session\n",
        "    :param bias_encoding: bool. Whether use bias encoding or postional encoding\n",
        "    :param att_embedding_size: positive int, the embedding size of each attention head\n",
        "    :param att_head_num: positive int, the number of attention head\n",
        "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n",
        "    :param dnn_activation: Activation function to use in deep net\n",
        "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
        "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net\n",
        "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
        "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
        "    :param init_std: float,to use as the initialize std of embedding vector\n",
        "    :param seed: integer ,to use as random seed.\n",
        "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
        "    :return: A Keras model instance.\n",
        "\n",
        "    \"\"\"\n",
        "    check_feature_config_dict(feature_dim_dict)\n",
        "\n",
        "    if (att_embedding_size * att_head_num != len(sess_feature_list) * embedding_size):\n",
        "        raise ValueError(\n",
        "            \"len(session_feature_lsit) * embedding_size must equal to att_embedding_size * att_head_num ,got %d * %d != %d *%d\" % (\n",
        "            len(sess_feature_list), embedding_size, att_embedding_size, att_head_num))\n",
        "\n",
        "    sparse_input, dense_input, user_behavior_input_dict, _, user_sess_length = get_input(\n",
        "        feature_dim_dict, sess_feature_list, sess_max_count, sess_len_max)\n",
        "\n",
        "    sparse_embedding_dict = {feat.name: Embedding(feat.dimension, embedding_size,\n",
        "                                                  embeddings_initializer=RandomNormal(\n",
        "                                                      mean=0.0, stddev=init_std, seed=seed),\n",
        "                                                  embeddings_regularizer=l2(\n",
        "                                                      l2_reg_embedding),\n",
        "                                                  name='sparse_emb_' +\n",
        "                                                       str(i) + '-' + feat.name,\n",
        "                                                  mask_zero=(feat.name in sess_feature_list)) for i, feat in\n",
        "                             enumerate(feature_dim_dict[\"sparse\"])}\n",
        "\n",
        "    query_emb_list = get_embedding_vec_list(sparse_embedding_dict, sparse_input, feature_dim_dict[\"sparse\"],\n",
        "                                            sess_feature_list, sess_feature_list)\n",
        "\n",
        "    query_emb = concat_fun(query_emb_list)\n",
        "\n",
        "    deep_input_emb_list = get_embedding_vec_list(sparse_embedding_dict, sparse_input, feature_dim_dict[\"sparse\"],\n",
        "                                                 mask_feat_list=sess_feature_list)\n",
        "    deep_input_emb = concat_fun(deep_input_emb_list)\n",
        "    deep_input_emb = Flatten()(NoMask()(deep_input_emb))\n",
        "\n",
        "    tr_input = sess_interest_division(sparse_embedding_dict, user_behavior_input_dict, feature_dim_dict['sparse'],\n",
        "                                      sess_feature_list, sess_max_count, bias_encoding=bias_encoding)\n",
        "\n",
        "    Self_Attention = Transformer(att_embedding_size, att_head_num, dropout_rate=0, use_layer_norm=False,\n",
        "                                 use_positional_encoding=(not bias_encoding), seed=seed, supports_masking=True,\n",
        "                                 blinding=True)\n",
        "    sess_fea = sess_interest_extractor(\n",
        "        tr_input, sess_max_count, Self_Attention)\n",
        "\n",
        "    interest_attention_layer = AttentionSequencePoolingLayer(att_hidden_units=(64, 16), weight_normalization=True,\n",
        "                                                             supports_masking=False)(\n",
        "        [query_emb, sess_fea, user_sess_length])\n",
        "\n",
        "    lstm_outputs = BiLSTM(len(sess_feature_list) * embedding_size,\n",
        "                          layers=2, res_layers=0, dropout_rate=0.2, )(sess_fea)\n",
        "    lstm_attention_layer = AttentionSequencePoolingLayer(att_hidden_units=(64, 16), weight_normalization=True)(\n",
        "        [query_emb, lstm_outputs, user_sess_length])\n",
        "\n",
        "    deep_input_emb = Concatenate()(\n",
        "        [deep_input_emb, Flatten()(interest_attention_layer), Flatten()(lstm_attention_layer)])\n",
        "    if len(dense_input) > 0:\n",
        "        deep_input_emb = Concatenate()(\n",
        "            [deep_input_emb] + list(dense_input.values()))\n",
        "\n",
        "    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn,\n",
        "                 dnn_dropout, dnn_use_bn, seed)(deep_input_emb)\n",
        "    output = Dense(1, use_bias=False, activation=None)(output)\n",
        "    output = PredictionLayer(task)(output)\n",
        "\n",
        "    sess_input_list = []\n",
        "    # sess_input_length_list = []\n",
        "    for i in range(sess_max_count):\n",
        "        sess_name = \"sess_\" + str(i)\n",
        "        sess_input_list.extend(get_inputs_list(\n",
        "            [user_behavior_input_dict[sess_name]]))\n",
        "        # sess_input_length_list.append(user_behavior_length_dict[sess_name])\n",
        "\n",
        "    model_input_list = get_inputs_list([sparse_input, dense_input]) + sess_input_list + [\n",
        "        user_sess_length]\n",
        "\n",
        "    model = Model(inputs=model_input_list, outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_input(feature_dim_dict, seq_feature_list, sess_max_count, seq_max_len):\n",
        "    sparse_input, dense_input = create_singlefeat_inputdict(feature_dim_dict)\n",
        "    user_behavior_input = {}\n",
        "    for idx in range(sess_max_count):\n",
        "        sess_input = OrderedDict()\n",
        "        for i, feat in enumerate(seq_feature_list):\n",
        "            sess_input[feat] = Input(\n",
        "                shape=(seq_max_len,), name='seq_' + str(idx) + str(i) + '-' + feat)\n",
        "\n",
        "        user_behavior_input[\"sess_\" + str(idx)] = sess_input\n",
        "\n",
        "    user_behavior_length = {\"sess_\" + str(idx): Input(shape=(1,), name='seq_length' + str(idx)) for idx in\n",
        "                            range(sess_max_count)}\n",
        "    user_sess_length = Input(shape=(1,), name='sess_length')\n",
        "\n",
        "    return sparse_input, dense_input, user_behavior_input, user_behavior_length, user_sess_length\n",
        "\n",
        "\n",
        "def sess_interest_division(sparse_embedding_dict, user_behavior_input_dict, sparse_fg_list, sess_feture_list,\n",
        "                           sess_max_count,\n",
        "                           bias_encoding=True):\n",
        "    tr_input = []\n",
        "    for i in range(sess_max_count):\n",
        "        sess_name = \"sess_\" + str(i)\n",
        "        keys_emb_list = get_embedding_vec_list(sparse_embedding_dict, user_behavior_input_dict[sess_name],\n",
        "                                               sparse_fg_list, sess_feture_list, sess_feture_list)\n",
        "        # [sparse_embedding_dict[feat](user_behavior_input_dict[sess_name][feat]) for feat in\n",
        "        #             sess_feture_list]\n",
        "        keys_emb = concat_fun(keys_emb_list)\n",
        "        tr_input.append(keys_emb)\n",
        "    if bias_encoding:\n",
        "        tr_input = BiasEncoding(sess_max_count)(tr_input)\n",
        "    return tr_input\n",
        "\n",
        "\n",
        "def sess_interest_extractor(tr_input, sess_max_count, TR):\n",
        "    tr_out = []\n",
        "    for i in range(sess_max_count):\n",
        "        tr_out.append(TR(\n",
        "            [tr_input[i], tr_input[i]]))\n",
        "    sess_fea = concat_fun(tr_out, axis=1)\n",
        "    return sess_fea\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}